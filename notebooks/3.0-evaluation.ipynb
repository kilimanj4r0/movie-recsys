{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the evaluation step, I calculate metrics for `PureSVD` model on the A and B splits of the dataset. Also, I average metrics across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ua.base', 'ua.test', 'ub.base', 'ub.test']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "data_interim_dir = '../benchmark/data/'\n",
    "data_filenames = [f'u{t}.{split}' for t in ['a', 'b'] for split in ['base', 'test']]\n",
    "data_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ua', 'ub'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "data = {}\n",
    "for i in range(0, len(data_filenames), 2):\n",
    "    base_filename, test_filename = data_filenames[i:i+2]\n",
    "    data_title = base_filename.split('.')[0]\n",
    "    with open(data_interim_dir + base_filename + '.pickle', 'rb') as base:\n",
    "        with open(data_interim_dir + test_filename + '.pickle', 'rb') as test:\n",
    "            with open(data_interim_dir + base_filename + '.df.pickle', 'rb') as base_df:\n",
    "                with open(data_interim_dir + test_filename + '.df.pickle', 'rb') as test_df:\n",
    "                    data[data_title] = (pickle.load(base), pickle.load(test), pickle.load(base_df), pickle.load(test_df))\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'../models/best_model.pickle'\n",
    "\n",
    "with open(model_path, 'rb') as pickle_file:\n",
    "    best_model = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on A and B splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I compute Classification and Ranking metrics [implemeted in RecTools](https://rectools.readthedocs.io/en/stable/api/rectools.metrics.html), namely:\n",
    "- F1Beta\n",
    "- Normalized DCG\n",
    "- Mean Average Precision\n",
    "- Mean Reciprocal Rank\n",
    "- Serendipity (added for the interest)\n",
    "\n",
    "For all metrics, $k=10$ is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectools.metrics.scoring import calc_metrics\n",
    "from rectools.metrics import F1Beta, NDCG, MAP, MRR, Serendipity\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    f'F1Beta@{K}': F1Beta(k=K),  # Classification\n",
    "    f'NDCG@{K}': NDCG(k=K, log_base=3),  # Ranking\n",
    "    f'MAP@{K}': MAP(k=K),  # Ranking\n",
    "    f'MRR@{K}': MRR(k=K),  # Ranking\n",
    "    f'Serendipity@{K}': Serendipity(k=K),  # Serendipity: novelty and relevance together\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fold):\n",
    "    return data[f'u{fold}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectools import Columns\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, fold_data):\n",
    "    train, test, train_df, test_df = fold_data\n",
    "    model.fit(train)\n",
    "    recos = model.recommend(\n",
    "        users=train_df[Columns.User].unique(),\n",
    "        dataset=train,\n",
    "        k=K,\n",
    "        filter_viewed=True,\n",
    "    )\n",
    "    return calc_metrics(\n",
    "        metrics,\n",
    "        reco=recos,\n",
    "        interactions=test_df,\n",
    "        prev_interactions=train_df,\n",
    "        catalog=train_df[Columns.Item].unique()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def print_metrics_table(metrics_dict):\n",
    "    table = []\n",
    "\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        table.append([metric_name, metric_value])\n",
    "\n",
    "    print(tabulate(table))\n",
    "\n",
    "\n",
    "def test_model(model, folds=['a', 'b']):\n",
    "    total_metrics = {k: 0 for k in metrics.keys()}\n",
    "    for fold in folds:\n",
    "        fold_data = load_data(fold)\n",
    "        fold_metrics = train_and_evaluate(model, fold_data)\n",
    "        for metric_name, metric_value in fold_metrics.items():\n",
    "            total_metrics[metric_name] += metric_value\n",
    "        \n",
    "        print(f\"Fold {fold}:\")\n",
    "        print_metrics_table(fold_metrics)\n",
    "        print()\n",
    "    \n",
    "    average_metrics = {metric_name: total_value / len(folds) for metric_name, total_value in total_metrics.items()}\n",
    "    print(f\"Average across test folds:\")\n",
    "    print_metrics_table(average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold a:\n",
      "--------------  ----------\n",
      "F1Beta@10       0.242842\n",
      "NDCG@10         0.285174\n",
      "MRR@10          0.613322\n",
      "MAP@10          0.149509\n",
      "Serendipity@10  0.00599713\n",
      "--------------  ----------\n",
      "\n",
      "Fold b:\n",
      "--------------  ----------\n",
      "F1Beta@10       0.231601\n",
      "NDCG@10         0.273182\n",
      "MRR@10          0.592713\n",
      "MAP@10          0.141989\n",
      "Serendipity@10  0.00599345\n",
      "--------------  ----------\n",
      "\n",
      "Average across test folds:\n",
      "--------------  ----------\n",
      "F1Beta@10       0.237222\n",
      "NDCG@10         0.279178\n",
      "MAP@10          0.145749\n",
      "MRR@10          0.603017\n",
      "Serendipity@10  0.00599529\n",
      "--------------  ----------\n"
     ]
    }
   ],
   "source": [
    "test_model(best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
