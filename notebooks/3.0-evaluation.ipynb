{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the evaluation step, I calculate metrics for `PureSVD` model on the A and B splits of the dataset. Also, I average metrics across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ua.base', 'ua.test', 'ub.base', 'ub.test']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "data_interim_dir = '../data/interim/'\n",
    "test_data_dir = '../benchmark/data/'\n",
    "data_filenames = [f'u{t}.{split}' for t in ['a', 'b'] for split in ['base', 'test']]\n",
    "data_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ua', 'ub'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "data = {}\n",
    "for i in range(0, len(data_filenames), 2):\n",
    "    base_filename, test_filename = data_filenames[i:i+2]\n",
    "    data_title = base_filename.split('.')[0]\n",
    "    with open(test_data_dir + base_filename + '.pickle', 'rb') as base:\n",
    "        with open(test_data_dir + test_filename + '.pickle', 'rb') as test:\n",
    "            with open(test_data_dir + base_filename + '.df.pickle', 'rb') as base_df:\n",
    "                with open(test_data_dir + test_filename + '.df.pickle', 'rb') as test_df:\n",
    "                    data[data_title] = (pickle.load(base), pickle.load(test), pickle.load(base_df), pickle.load(test_df))\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'../models/best_model.pickle'\n",
    "\n",
    "with open(model_path, 'rb') as pickle_file:\n",
    "    best_model = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on A and B splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I compute Classification and Ranking metrics [implemeted in RecTools](https://rectools.readthedocs.io/en/stable/api/rectools.metrics.html), namely:\n",
    "- F1Beta\n",
    "- Normalized DCG\n",
    "- Mean Average Precision\n",
    "- Mean Reciprocal Rank\n",
    "- Serendipity (added for the interest)\n",
    "\n",
    "For all metrics, $k=10$ is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectools.metrics.scoring import calc_metrics\n",
    "from rectools.metrics import F1Beta, MAP, MRR, Serendipity, MeanInvUserFreq\n",
    "\n",
    "\n",
    "metrics_name = {\n",
    "    'F1Beta': F1Beta,\n",
    "    'MRR': MRR,\n",
    "    'MAP': MAP,\n",
    "    'Novelty': MeanInvUserFreq,\n",
    "    'Serendipity': Serendipity\n",
    "}\n",
    "metrics = {}\n",
    "for metric_name, metric in metrics_name.items():\n",
    "    for k in (1, 5, 10):\n",
    "        metrics[f'{metric_name}@{k}'] = metric(k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fold):\n",
    "    return data[f'u{fold}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectools import Columns\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, fold_data, fold=None):\n",
    "    train, test, train_df, test_df = fold_data\n",
    "    model.fit(train)\n",
    "    recos = model.recommend(\n",
    "        users=train_df[Columns.User].unique(),\n",
    "        dataset=train,\n",
    "        k=K,\n",
    "        filter_viewed=True,\n",
    "    )\n",
    "    if fold is not None:\n",
    "        recos.to_csv(data_interim_dir + 'recos_' + fold + '.csv')\n",
    "    return calc_metrics(\n",
    "        metrics,\n",
    "        reco=recos,\n",
    "        interactions=test_df,\n",
    "        prev_interactions=train_df,\n",
    "        catalog=train_df[Columns.Item].unique()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def print_metrics_table(metrics_dict):\n",
    "    table = []\n",
    "\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        table.append([metric_name, metric_value])\n",
    "\n",
    "    print(tabulate(table, headers=['Metric', 'Value'], tablefmt='pretty'))\n",
    "\n",
    "\n",
    "def test_model(model, save_recos=False, folds=['a', 'b']):\n",
    "    total_metrics = {k: 0 for k in metrics.keys()}\n",
    "    for fold in folds:\n",
    "        fold_data = load_data(fold)\n",
    "        if save_recos:\n",
    "            fold_metrics = train_and_evaluate(model, fold_data, fold)\n",
    "        else:\n",
    "            fold_metrics = train_and_evaluate(model, fold_data, fold)\n",
    "        for metric_name, metric_value in fold_metrics.items():\n",
    "            total_metrics[metric_name] += metric_value\n",
    "        \n",
    "        print(f\"Fold {fold}:\")\n",
    "        print_metrics_table(fold_metrics)\n",
    "        print()\n",
    "    \n",
    "    average_metrics = {metric_name: total_value / len(folds) for metric_name, total_value in total_metrics.items()}\n",
    "    print(f\"Average across test folds:\")\n",
    "    print_metrics_table(average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold a:\n",
      "+----------------+-----------------------+\n",
      "|     Metric     |         Value         |\n",
      "+----------------+-----------------------+\n",
      "|    F1Beta@1    |  0.08387158970403934  |\n",
      "|    F1Beta@5    |  0.21124072110286318  |\n",
      "|   F1Beta@10    |  0.24284199363732767  |\n",
      "|     MRR@1      |  0.46129374337221635  |\n",
      "|     MRR@5      |  0.6001767408978438   |\n",
      "|     MRR@10     |  0.6133220555808042   |\n",
      "|     MAP@1      |  0.04612937433722163  |\n",
      "|     MAP@5      |  0.11619476846942381  |\n",
      "|     MAP@10     |  0.1495090811156559   |\n",
      "|   Novelty@1    |   1.622449717317695   |\n",
      "|   Novelty@5    |   1.828821999261622   |\n",
      "|   Novelty@10   |  1.9779028021482392   |\n",
      "| Serendipity@1  | 0.0067597081250315594 |\n",
      "| Serendipity@5  | 0.006558602231985053  |\n",
      "| Serendipity@10 | 0.005997134272584964  |\n",
      "+----------------+-----------------------+\n",
      "\n",
      "Fold b:\n",
      "+----------------+----------------------+\n",
      "|     Metric     |        Value         |\n",
      "+----------------+----------------------+\n",
      "|    F1Beta@1    | 0.08117227417333463  |\n",
      "|    F1Beta@5    | 0.19964651820431245  |\n",
      "|   F1Beta@10    | 0.23160127253446447  |\n",
      "|     MRR@1      | 0.44644750795334043  |\n",
      "|     MRR@5      |  0.5773418168964299  |\n",
      "|     MRR@10     |  0.5927128044572371  |\n",
      "|     MAP@1      | 0.04464475079533403  |\n",
      "|     MAP@5      | 0.11061859314245316  |\n",
      "|     MAP@10     | 0.14198854550657308  |\n",
      "|   Novelty@1    |  1.6122850849346215  |\n",
      "|   Novelty@5    |  1.831119683698204   |\n",
      "|   Novelty@10   |  1.975496959304876   |\n",
      "| Serendipity@1  | 0.006434212817144395 |\n",
      "| Serendipity@5  | 0.006533736408097374 |\n",
      "| Serendipity@10 | 0.00599344739716054  |\n",
      "+----------------+----------------------+\n",
      "\n",
      "Average across test folds:\n",
      "+----------------+----------------------+\n",
      "|     Metric     |        Value         |\n",
      "+----------------+----------------------+\n",
      "|    F1Beta@1    | 0.08252193193868698  |\n",
      "|    F1Beta@5    | 0.20544361965358782  |\n",
      "|   F1Beta@10    | 0.23722163308589606  |\n",
      "|     MRR@1      | 0.45387062566277836  |\n",
      "|     MRR@5      |  0.5887592788971369  |\n",
      "|     MRR@10     |  0.6030174300190206  |\n",
      "|     MAP@1      | 0.045387062566277836 |\n",
      "|     MAP@5      | 0.11340668080593849  |\n",
      "|     MAP@10     |  0.1457488133111145  |\n",
      "|   Novelty@1    |  1.6173674011261583  |\n",
      "|   Novelty@5    |  1.829970841479913   |\n",
      "|   Novelty@10   |  1.9766998807265574  |\n",
      "| Serendipity@1  | 0.006596960471087977 |\n",
      "| Serendipity@5  | 0.006546169320041213 |\n",
      "| Serendipity@10 | 0.005995290834872752 |\n",
      "+----------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "test_model(best_model, save_recos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
